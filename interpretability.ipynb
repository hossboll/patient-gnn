{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import torch # type: ignore\n",
    "from models import SAGEnorm, GATnorm, GraphTransformernorm\n",
    "import os\n",
    "from torch.optim.lr_scheduler import StepLR # type: ignore\n",
    "from datetime import date\n",
    "import seaborn as sns\n",
    "from scipy.stats import mode\n",
    "import networkx as nx\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GT focal and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(222)\n",
    "H = load_trans()\n",
    "train_loader, test_loader, val_loader, dataset_loader = mask_and_batch_tran_interpret(H)\n",
    "\n",
    "def create_model_loss(config, loss_type=\"bce\", alpha=None, gamma=None):\n",
    "    if config[\"model_type\"] == 'gat':\n",
    "        model = GATnorm(config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"], config[\"activation_function\"], config[\"num_heads\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "    elif config[\"model_type\"] == 'graphsage':\n",
    "        model = SAGEnorm(config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"], config[\"activation_function\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "    elif config[\"model_type\"] == 'graphtransformer':\n",
    "        model = GraphTransformernorm(config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"], config[\"activation_function\"], config[\"num_heads\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "device = 'cpu'\n",
    "#'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading special loss models\n",
    "def load_checkpoint(basemodel_path, checkpoint_path, test_loader=None, load_state_dicts=True, loss_type='focal', alpha=0.75, gamma=1, device='cpu'):\n",
    "    base_model = torch.load(basemodel_path) #, map_location=device\n",
    "    print(base_model[\"config\"])\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path) #, map_location=device\n",
    "    print(checkpoint[\"config\"][\"loss_type\"], checkpoint[\"config\"][\"alpha\"])\n",
    "\n",
    "    model_loaded = create_model_loss(base_model[\"config\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "\n",
    "    if load_state_dicts:\n",
    "        model_loaded.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    optimizer = set_optim(base_model[\"config\"], model_loaded)\n",
    "\n",
    "    if load_state_dicts:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    if load_state_dicts:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "    model_loaded.to(device)\n",
    "    model_loaded.eval()\n",
    "\n",
    "    # first_batch = next(iter(test_loader))\n",
    "    # with torch.inference_mode():\n",
    "    #     first_batch.to(device)\n",
    "    #     loaded_model_output = model_loaded(first_batch)\n",
    "\n",
    "    return model_loaded, optimizer, scheduler #, loaded_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel_path = r\"BASEMODEL_PATH\"\n",
    "checkpoint_path = r\"CHECKPOINT_PATH\" \n",
    "model_loaded, optimizer, scheduler = load_checkpoint(basemodel_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "graph_filepath = r\"SIMILARITY_GRAPH_PATH\"\n",
    "\n",
    "with open(graph_filepath, 'rb') as file:\n",
    "            G = pkl.load(file)\n",
    "\n",
    "type(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_ids(loader):\n",
    "    # get patient ids\n",
    "    set_seed(22)\n",
    "    return np.array(get_unmasked_node_ids(loader))\n",
    "\n",
    "def find_node_by_patient_id(patient_id, graph):\n",
    "    # find node in the graph based on its patient id\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data.get('patient_id') == patient_id:\n",
    "            return node\n",
    "    return None\n",
    "\n",
    "def get_attention_weights(model, loader):\n",
    "    # get attention weights from the last layer (attention: modify model class so it can output attention weights)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        logits, (indices, attention_weights) = model(next(iter(loader))) #single graph batch\n",
    "\n",
    "    edge_index_np = indices.detach().cpu().numpy().T\n",
    "    attention_scores_np = attention_weights.detach().cpu().numpy()\n",
    "    \n",
    "    return edge_index_np, attention_scores_np\n",
    "\n",
    "def get_attention_scores(patient_id, graph, model, loader):\n",
    "    # return attention scores of a source node and its neighbors, based on its patient id\n",
    "    node_index = find_node_by_patient_id(patient_id, graph)\n",
    "    if node_index is None:\n",
    "        print(f\"Patient ID {patient_id} not found in the graph.\")\n",
    "        return []\n",
    "    edge_index_np, attention_scores_np = get_attention_weights(model, loader)\n",
    "    \n",
    "    info = []\n",
    "    neighbors = list(graph.neighbors(node_index))\n",
    "    print(f\"Patient ID: {patient_id}, Node index: {node_index}\")\n",
    "    print(f\"Neighbors: {neighbors}\")\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_patient_id = graph.nodes[neighbor].get('patient_id')\n",
    "        edge = (node_index, neighbor)\n",
    "        \n",
    "        # only consider the edge if node_index is the source (ORDER MATTERS)\n",
    "        mask = (edge_index_np[:, 0] == node_index) & (edge_index_np[:, 1] == neighbor)\n",
    "\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        print(f\"Considering edge: {edge}, Neighbor patient ID: {neighbor_patient_id}\")\n",
    "        #print(f\"Mask: {mask}\")\n",
    "\n",
    "        attention_score_indices = np.where(mask)[0]\n",
    "        print(f\"Attention score indices: {attention_score_indices}\")\n",
    "\n",
    "        for attention_score_index in attention_score_indices:\n",
    "            score = attention_scores_np[attention_score_index].item()\n",
    "            #hops = nx.shortest_path_length(graph, source=node_index, target=neighbor)\n",
    "            print(f\"Appending: source node index: {node_index}, target node index: {neighbor}, Attention score: {score}\") #, Hops: {hops}\")\n",
    "\n",
    "            info.append({\n",
    "                'source_node_index': node_index,\n",
    "                'target_node_index': neighbor,\n",
    "                'source_patient_id': patient_id,\n",
    "                'target_patient_id': neighbor_patient_id,\n",
    "                'attention_score': score,\n",
    "                #'hops': hops\n",
    "            })\n",
    "\n",
    "    return info\n",
    "    \n",
    "\n",
    "def profile_attention(results, loader, model, graph):\n",
    "    # match attention info with preds, probs, label based on patient_id, for all nodes in a loader\n",
    "    patient_ids = get_patient_ids(loader)\n",
    "    attention_profiles = {}\n",
    "\n",
    "    for key, instances in results.items(): # profile ; node info (dict)\n",
    "        if key not in attention_profiles:\n",
    "            attention_profiles[key] = {\n",
    "                'patient_ids': [],\n",
    "                'attention_weights': [],\n",
    "                'probs': [],\n",
    "                'preds': [],\n",
    "                'labels': []\n",
    "            }\n",
    "\n",
    "        for instance in instances:\n",
    "            patient_id = patient_ids[instance['index']]\n",
    "            attention_scores = get_attention_scores(patient_id, graph, model, loader)\n",
    "\n",
    "            attention_profiles[key]['patient_ids'].append(patient_id)\n",
    "            attention_profiles[key]['attention_weights'].extend(attention_scores)\n",
    "            attention_profiles[key]['probs'].append(instance['prob'])\n",
    "            attention_profiles[key]['preds'].append(instance['pred'])\n",
    "            attention_profiles[key]['labels'].append(instance['label'])\n",
    "\n",
    "    return attention_profiles\n",
    "\n",
    "\n",
    "def aggregate_attention_profiles(attention_profiles):\n",
    "    # get mean attention score for each profile\n",
    "    aggregated_profiles = {}\n",
    "    for key, profiles in attention_profiles.items():\n",
    "        attention_scores = []\n",
    "        for profile in profiles['attention_weights']:\n",
    "            attention_scores.append(profile['attention_score'])\n",
    "        aggregated_profiles[key] = np.mean(attention_scores) if attention_scores else 0\n",
    "    return aggregated_profiles\n",
    "\n",
    "def compute_statistics(attention_profiles, graph):\n",
    "    # stats from each profile\n",
    "    statistics = {}\n",
    "    for key, profiles in attention_profiles.items():\n",
    "        #print(profiles)\n",
    "        degrees = []\n",
    "        similarities = []\n",
    "        for profile in profiles['attention_weights']:\n",
    "            node_index = find_node_by_patient_id(profile['source_patient_id'], graph)\n",
    "            if node_index is not None:\n",
    "                degrees.append(graph.degree[node_index])\n",
    "                for neighbor in graph.neighbors(node_index):\n",
    "                    edge = (node_index, neighbor)\n",
    "                    if graph.has_edge(*edge):\n",
    "                        edge_feature = graph.edges[edge].get('weight', 0) # weight=0 if not specified\n",
    "                        similarities.append(edge_feature)\n",
    "        \n",
    "        if degrees:\n",
    "            degree_mean = np.mean(degrees)\n",
    "            degree_std = np.std(degrees)\n",
    "            degree_mode = mode(degrees)[0][0] if degrees else None\n",
    "            degree_median = np.median(degrees)\n",
    "        else:\n",
    "            degree_mean = degree_std = degree_mode = 0\n",
    "\n",
    "        if similarities:\n",
    "            similarity_mean = np.mean(similarities)\n",
    "            similarity_std = np.std(similarities)\n",
    "            similarity_mode = mode(similarities)[0][0] if similarities else None\n",
    "            similarity_median = np.median(similarities)\n",
    "        else:\n",
    "            similarity_mean = similarity_std = similarity_mode = 0\n",
    "\n",
    "        statistics[key] = {\n",
    "            'average_degree': degree_mean,\n",
    "            'degree_std': degree_std,\n",
    "            'degree_mode': degree_mode,\n",
    "            'degree_median': degree_median,\n",
    "            'average_similarity': similarity_mean,\n",
    "            'similarity_std': similarity_std,\n",
    "            'similarity_mode': similarity_mode,\n",
    "            'similarity_median': similarity_median,\n",
    "        }\n",
    "    return statistics\n",
    "\n",
    "def visualize_statistics(statistics):\n",
    "    for key, stats in statistics.items():\n",
    "        print(f\"Statistics for {key}:\")\n",
    "        print(f\"  Average degree: {stats['average_degree']:.5f}\")\n",
    "        print(f\"  Degree sd: {stats['degree_std']:.5f}\")\n",
    "        print(f\"  Degree mode: {stats['degree_mode']:.5f}\")\n",
    "        print(f\"  Degree median: {stats['degree_median']:.5f}\")\n",
    "        print(f\"  Avg similarity: {stats['average_similarity']:.5f}\")\n",
    "        print(f\"  Similarity sd: {stats['similarity_std']:.5f}\")\n",
    "        print(f\"  Similarity mode: {stats['similarity_mode']:.5f}\")\n",
    "        print(f\"  Similarity median: {stats['similarity_median']:.5f}\")\n",
    "\n",
    "def visualize_attention_profiles(aggregated_profiles):\n",
    "    labels = list(aggregated_profiles.keys())\n",
    "    weights = list(aggregated_profiles.values())\n",
    "\n",
    "    sorted_indices = np.argsort(weights)\n",
    "    sorted_labels = [labels[i] for i in sorted_indices]\n",
    "    sorted_weights = [weights[i] for i in sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bar_positions = np.arange(len(sorted_labels))\n",
    "    bar_width = 0.4\n",
    "\n",
    "    plt.bar(bar_positions, sorted_weights, bar_width, label='Attention weights')\n",
    "\n",
    "    for i, weight in enumerate(sorted_weights):\n",
    "        plt.text(i, weight * 0.95, f'{weight:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.title('Average attention weights')\n",
    "    plt.xlabel('Profiles')\n",
    "    plt.ylabel('Attention weight')\n",
    "    plt.xticks(bar_positions, sorted_labels)\n",
    "    plt.ylim(0, max(sorted_weights) * 1.1)  \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_distribution(attention_profiles):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for key, profiles in attention_profiles.items():\n",
    "        attention_scores = [p['attention_score'] for p in profiles['attention_weights']]\n",
    "        sns.kdeplot(attention_scores, label=key, shade=True)\n",
    "    \n",
    "    plt.title('Attention score distribution by profile')\n",
    "    plt.xlabel('Attention score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def detailed_report(attention_profiles, graph, patient_ids=None):\n",
    "    if patient_ids is not None:\n",
    "        filtered_attention_profiles = {}\n",
    "        for key, profile in attention_profiles.items():\n",
    "            filtered_weights = [p for p in profile['attention_weights'] if p['source_patient_id'] in patient_ids]\n",
    "            if filtered_weights:\n",
    "                filtered_profile = {\n",
    "                    'patient_ids': [pid for pid in profile['patient_ids'] if pid in patient_ids],\n",
    "                    'attention_weights': filtered_weights,\n",
    "                    'probs': [profile['probs'][i] for i, pid in enumerate(profile['patient_ids']) if pid in patient_ids],\n",
    "                    'preds': [profile['preds'][i] for i, pid in enumerate(profile['patient_ids']) if pid in patient_ids],\n",
    "                    'labels': [profile['labels'][i] for i, pid in enumerate(profile['patient_ids']) if pid in patient_ids]\n",
    "                }\n",
    "                filtered_attention_profiles[key] = filtered_profile\n",
    "        attention_profiles = filtered_attention_profiles\n",
    "    \n",
    "    aggregated_profiles = aggregate_attention_profiles(attention_profiles)\n",
    "    statistics = compute_statistics(attention_profiles, graph)\n",
    "    visualize_statistics(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return ALL instances from each profile: index, id, prob, pred, label\n",
    "def return_results(probs, preds, labels, loader): \n",
    "    set_seed(22)\n",
    "    results = {}\n",
    "    patient_ids = get_unmasked_node_ids(loader)\n",
    "\n",
    "    # true positives\n",
    "    true_positives = np.where((preds == 1) & (labels == 1))[0]\n",
    "    results['True positive (TP)'] = [{\n",
    "        'index': idx,\n",
    "        'patient_id': patient_ids[idx],\n",
    "        'prob': probs[idx],\n",
    "        'pred': preds[idx],\n",
    "        'label': labels[idx]\n",
    "    } for idx in true_positives]\n",
    "\n",
    "    # true negatives\n",
    "    true_negatives = np.where((preds == 0) & (labels == 0))[0]\n",
    "    results['True negative (TN)'] = [{\n",
    "        'index': idx,\n",
    "        'patient_id': patient_ids[idx],\n",
    "        'prob': probs[idx],\n",
    "        'pred': preds[idx],\n",
    "        'label': labels[idx]\n",
    "    } for idx in true_negatives]\n",
    "\n",
    "    # false positives\n",
    "    false_positives = np.where((preds == 1) & (labels == 0))[0]\n",
    "    results['False positive (FP)'] = [{\n",
    "        'index': idx,\n",
    "        'patient_id': patient_ids[idx],\n",
    "        'prob': probs[idx],\n",
    "        'pred': preds[idx],\n",
    "        'label': labels[idx]\n",
    "    } for idx in false_positives]\n",
    "\n",
    "    # false negatives\n",
    "    false_negatives = np.where((preds == 0) & (labels == 1))[0]\n",
    "    results['False negative (FN)'] = [{\n",
    "        'index': idx,\n",
    "        'patient_id': patient_ids[idx],\n",
    "        'prob': probs[idx],\n",
    "        'pred': preds[idx],\n",
    "        'label': labels[idx]\n",
    "    } for idx in false_negatives]\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_predictions(dataset_loader):\n",
    "    set_seed(22)\n",
    "    _, _, probs, preds, labels = test(dataset_loader, model_loaded, device)\n",
    "    return np.array(probs), np.array(preds), np.array(labels)\n",
    "\n",
    "# remember - for ALL instances (train, test, val)\n",
    "probs,preds,labels = get_predictions(dataset_loader)\n",
    "results = return_results(probs, preds, labels, dataset_loader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('attention_profiles.pkl'):\n",
    "    with open('attention_profiles.pkl', 'rb') as pickle_file:\n",
    "        attention_profiles = pkl.load(pickle_file)\n",
    "else:\n",
    "    attention_profiles = profile_attention(results, test_loader, model_loaded, G) # this is with test loader. later, full dataset+filtering\n",
    "    with open('attention_profiles.pkl', 'wb') as pickle_file:\n",
    "        pkl.dump(attention_profiles, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_report(attention_profiles, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_distribution(attention_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get info from full graph; then filter and plot test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import ModuleList, Linear, BatchNorm1d\n",
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "#rerun \"checkpoint load\"\n",
    "class GraphTransformernorm(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, dropout, activation_function, num_heads, loss_type=\"bce\", alpha=0, gamma=0):\n",
    "        super(GraphTransformernorm, self).__init__()\n",
    "        torch.manual_seed(22)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.loss_type = loss_type\n",
    "        self.input_size = 300\n",
    "\n",
    "        if activation_function == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation_function == 'leaky_relu':\n",
    "            self.activation = F.leaky_relu\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.bns = ModuleList()  \n",
    "        self.convs.append(TransformerConv(self.input_size, hidden_size, heads=self.num_heads))\n",
    "        self.bns.append(BatchNorm1d(hidden_size * self.num_heads))\n",
    "        for _ in range(1, self.num_layers - 1):\n",
    "            self.convs.append(TransformerConv(hidden_size * self.num_heads, hidden_size, heads=self.num_heads))\n",
    "            self.bns.append(BatchNorm1d(hidden_size * self.num_heads))\n",
    "        self.convs.append(TransformerConv(hidden_size * self.num_heads, hidden_size, heads=1, concat=False))\n",
    "        self.bns.append(BatchNorm1d(hidden_size))  \n",
    "\n",
    "        self.post_mp = Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.node_feature, data.edge_index, data.batch\n",
    "\n",
    "        for i in range(len(self.convs) - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)  \n",
    "            x = self.activation(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x, attention_weights = self.convs[-1](x, edge_index, return_attention_weights= True) #,return_attention_weights= True\n",
    "        \n",
    "        x = self.bns[-1](x)  \n",
    "        x = self.post_mp(x)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('attention_profiles_fulldataset.pkl'):\n",
    "    with open('attention_profiles_fulldataset.pkl', 'rb') as pickle_file:\n",
    "        attention_profiles_fulldataset = pkl.load(pickle_file)\n",
    "else:\n",
    "    with open('attention_profiles_fulldataset.pkl', 'wb') as pickle_file:\n",
    "        attention_profiles_fulldataset = profile_attention(results, dataset_loader, model_loaded, G)\n",
    "        pkl.dump(attention_profiles_fulldataset, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient_ids = [pid for profiles in attention_profiles.values() for pid in profiles['patient_ids']] # getting ids from the test set\n",
    "print(test_patient_ids, len(test_patient_ids)) #952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11745 in test_patient_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_attention_profiles_by_test_ids(attention_profiles_fulldataset, test_patient_ids, results, graph):\n",
    "    filtered_profiles = {}\n",
    "    all_instances = [item for sublist in results.values() for item in sublist]\n",
    "\n",
    "    for key, profile in attention_profiles_fulldataset.items():\n",
    "        filtered_weights = [p for p in profile['attention_weights'] if p['source_patient_id'] in test_patient_ids] # keep source nodes that are in test set\n",
    "        if filtered_weights:\n",
    "            profile_test_patient_ids = [pid for pid in profile['patient_ids'] if pid in test_patient_ids]\n",
    "            filtered_profiles[key] = {\n",
    "                'patient_ids': profile_test_patient_ids,\n",
    "                'attention_weights': filtered_weights,\n",
    "                'probs': [instance['prob'] for instance in all_instances if instance['patient_id'] in profile_test_patient_ids],\n",
    "                'preds': [instance['pred'] for instance in all_instances if instance['patient_id'] in profile_test_patient_ids],\n",
    "                'labels': [instance['label'] for instance in all_instances if instance['patient_id'] in profile_test_patient_ids],\n",
    "                'neighbor_labels': []\n",
    "            }\n",
    "            for weight in filtered_weights:\n",
    "                source_node = find_node_by_patient_id(weight['source_patient_id'], graph)\n",
    "                if source_node is not None:\n",
    "                    neighbor_labels = []\n",
    "                    for neighbor in graph.neighbors(source_node):\n",
    "                        neighbor_patient_id = graph.nodes[neighbor].get('patient_id')\n",
    "                        neighbor_label = next((instance['label'] for instance in all_instances if instance['patient_id'] == neighbor_patient_id), None)\n",
    "                        if neighbor_label is not None:\n",
    "                            neighbor_labels.append(neighbor_label)\n",
    "                    filtered_profiles[key]['neighbor_labels'].append(neighbor_labels)\n",
    "    return filtered_profiles\n",
    "\n",
    "filtered_profiles = filter_attention_profiles_by_test_ids(attention_profiles_fulldataset, test_patient_ids, results, G)\n",
    "print(filtered_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(filtered_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_distribution(filtered_profiles):\n",
    "    label_colors = {0: '#3A49FF', 1: \"#FFF800\"}  \n",
    "\n",
    "    for key, profile in filtered_profiles.items():\n",
    "        attention_weights = profile['attention_weights']\n",
    "        neighbor_labels = profile['neighbor_labels']\n",
    "        \n",
    "        neighbor_label_distribution = {}\n",
    "        \n",
    "        # attention scores based on neighbor labels\n",
    "        for i, weights in enumerate(attention_weights):\n",
    "            attention_score = weights['attention_score']\n",
    "            if i < len(neighbor_labels):\n",
    "                for neighbor_label in neighbor_labels[i]:\n",
    "                    if neighbor_label not in neighbor_label_distribution:\n",
    "                        neighbor_label_distribution[neighbor_label] = []\n",
    "                    neighbor_label_distribution[neighbor_label].append(attention_score)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # kde for each neighbor label\n",
    "        for neighbor_label, scores in neighbor_label_distribution.items():\n",
    "            sns.kdeplot(scores, label=f'Neighbor label: {neighbor_label}', shade=True, color=label_colors[neighbor_label])\n",
    "        \n",
    "        plt.title(f'Attention score distribution for {key}')\n",
    "        plt.xlabel('Attention score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot_attention_distribution(filtered_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbor_label_distribution(filtered_profiles):\n",
    "    label_colors = {0: '#636EFA', 1: \"#E8E337\"}  \n",
    "\n",
    "    for key, profile in filtered_profiles.items():\n",
    "        neighbor_labels = profile['neighbor_labels']\n",
    "        \n",
    "        neighbor_label_counts = {0: 0, 1: 0}\n",
    "        \n",
    "        for labels in neighbor_labels:\n",
    "            for label in labels:\n",
    "                neighbor_label_counts[label] += 1\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(neighbor_label_counts.keys(), neighbor_label_counts.values(), color=[label_colors[label] for label in neighbor_label_counts.keys()], alpha=0.7)\n",
    "        \n",
    "        plt.xticks([0, 1], ['Label 0', 'Label 1'])\n",
    "        plt.title(f'Neighbor label distribution for {key}')\n",
    "        plt.xlabel('Neighbor label')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "plot_neighbor_label_distribution(filtered_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_report(attention_profiles_fulldataset, G, patient_ids=test_patient_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sweep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
