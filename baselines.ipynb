{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "Implementing ML algorithms to compare with the performance of the GT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(222)\n",
    "H = load_trans()\n",
    "train_loader, test_loader, val_loader = mask_and_batch_trans(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patient_ids = get_unmasked_node_ids(train_loader)\n",
    "test_patient_ids = get_unmasked_node_ids(test_loader)\n",
    "val_patient_ids = get_unmasked_node_ids(val_loader)\n",
    "len(train_patient_ids), len(test_patient_ids), len(val_patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = set(train_patient_ids)\n",
    "test_set = set(test_patient_ids)\n",
    "val_set = set(val_patient_ids)\n",
    "\n",
    "train_test_overlap = train_set.intersection(test_set)\n",
    "train_val_overlap = train_set.intersection(val_set)\n",
    "test_val_overlap = test_set.intersection(val_set)\n",
    "\n",
    "assert not train_test_overlap, \"There is an overlap between train and test patient IDs!\"\n",
    "assert not train_val_overlap, \"There is an overlap between train and validation patient IDs!\"\n",
    "assert not test_val_overlap, \"There is an overlap between test and validation patient IDs!\"\n",
    "\n",
    "print(\"No overlap between train, test, and validation patient IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = pd.read_csv(r\"/PATIENT_DATA.csv\")\n",
    "patient_data = patient_data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = patient_data[patient_data['patient_id'].isin(train_set)]\n",
    "test_df = patient_data[patient_data['patient_id'].isin(test_set)]\n",
    "val_df = patient_data[patient_data['patient_id'].isin(val_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['patient_id'])\n",
    "test_df = test_df.drop(columns=['patient_id'])\n",
    "val_df = val_df.drop(columns=['patient_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels for each set\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_val = val_df.drop('label', axis=1)\n",
    "y_val = val_df['label']\n",
    "\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search/plot metrics\n",
    "https://stackoverflow.com/questions/34624978/is-there-easy-way-to-grid-search-without-cross-validation-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, param_grid, X_train, y_train, X_val, y_val, X_test, y_test):    \n",
    "    param_candidates = ParameterGrid(param_grid)\n",
    "    print(f'{len(param_candidates)} candidates')\n",
    "\n",
    "    results = []\n",
    "    for i, params in enumerate(param_candidates):\n",
    "        model.set_params(**params).fit(X_train, y_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        score = f1_score(y_val, y_val_pred)  \n",
    "        results.append([params, score])\n",
    "        print(f'{i+1}/{len(param_candidates)}: ', params, score)\n",
    "\n",
    "    best_params, best_score = max(results, key=lambda x: x[1])\n",
    "    print(f'Best parameters: {best_params}')\n",
    "    print(f'Best validation F1 score: {best_score}')\n",
    "\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_f1_score = f1_score(y_test, y_test_pred)  \n",
    "    print(f'Test set F1 score: {test_f1_score}')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(model, X, y, set_name):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y, y_pred)\n",
    "    auroc = roc_auc_score(y, y_pred_proba)\n",
    "    auprc = average_precision_score(y, y_pred_proba)\n",
    "    recall = recall_score(y, y_pred, average='binary', zero_division=0)\n",
    "    precision = precision_score(y, y_pred, average='binary', zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Balanced accuracy\": balanced_accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision,\n",
    "        \"F1 Score\": f1,\n",
    "        \"AUROC\": auroc,\n",
    "        \"AUPRC\": auprc,\n",
    "        \"Confusion matrix\": cm\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15],    \n",
    "    'weights': ['uniform', 'distance'],     \n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  \n",
    "    'p': [1, 2] #manhatan, eucl\n",
    "}\n",
    "best_model_knn = grid_search(knn, knn_grid, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(best_model_knn, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(best_model_knn, '/KNN.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [100, 200]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 200]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['elasticnet'],\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['saga'],\n",
    "        'l1_ratio': [0.1, 0.5, 0.7, 1.0],\n",
    "        'max_iter': [100, 200]\n",
    "    },\n",
    "]\n",
    "best_model_log_reg = grid_search(log_reg, log_reg_grid, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(best_model_log_reg, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(best_model_log_reg, '/LR.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [5, 7],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "best_model_rf = grid_search(rf, rf_grid, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(best_model_rf, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#joblib.dump(best_model_rf, '/RF.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbt = GradientBoostingClassifier()\n",
    "gbt_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "best_model_gbt = grid_search(gbt, gbt_grid, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(best_model_gbt, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(best_model_gbt, '/GBT.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "mlp_grid = {\n",
    "    'hidden_layer_sizes': [(128, 64, 32), (150, 100, 50), (50, 30)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [200, 400]\n",
    "}\n",
    "best_model_mlp = grid_search(mlp, mlp_grid, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(best_model_mlp, X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(best_model_mlp, '/MLP.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "loaded_mlp = joblib.load('/nfs/home/heloss23/sweep/max-f1/baselines/mlp_model.joblib')\n",
    "loaded_knn = joblib.load('/nfs/home/heloss23/sweep/max-f1/baselines/knn_model.joblib')\n",
    "loaded_gbt = joblib.load('/nfs/home/heloss23/sweep/max-f1/baselines/gbt_model.joblib')\n",
    "loaded_rf = joblib.load('/nfs/home/heloss23/sweep/max-f1/baselines/rf_model.joblib')\n",
    "loaded_log_reg = joblib.load('/nfs/home/heloss23/sweep/max-f1/baselines/log_reg_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'KNN': loaded_knn,\n",
    "    'GBT': loaded_gbt,\n",
    "    'LR': loaded_log_reg,\n",
    "    'RF': loaded_rf,\n",
    "    'MLP': loaded_mlp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading special loss models\n",
    "\n",
    "def create_model_loss(config, loss_type=\"bce\", alpha=None, gamma=None):\n",
    "    if config[\"model_type\"] == 'gat':\n",
    "        model = GATnorm(config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"], config[\"activation_function\"], config[\"num_heads\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "    elif config[\"model_type\"] == 'graphsage':\n",
    "        model = SAGEnorm(config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"], config[\"activation_function\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "    elif config[\"model_type\"] == 'graphtransformer':\n",
    "        model = GraphTransformernorm(config[\"hidden_size\"], config[\"num_layers\"], config[\"dropout\"], config[\"activation_function\"], config[\"num_heads\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def load_checkpoint(basemodel_path, checkpoint_path, test_loader, load_state_dicts=True, loss_type='focal', alpha=0.75, gamma=1, device='cuda'):\n",
    "    base_model = torch.load(basemodel_path) #, map_location=device\n",
    "    print(base_model[\"config\"])\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path) #, map_location=device\n",
    "    print(checkpoint[\"config\"][\"loss_type\"], checkpoint[\"config\"][\"alpha\"])\n",
    "\n",
    "    model_loaded = create_model_loss(base_model[\"config\"], loss_type=loss_type, alpha=alpha, gamma=gamma)\n",
    "\n",
    "    if load_state_dicts:\n",
    "        model_loaded.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    optimizer = set_optim(base_model[\"config\"], model_loaded)\n",
    "\n",
    "    if load_state_dicts:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "    if load_state_dicts:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "    model_loaded.to(device)\n",
    "    model_loaded.eval()\n",
    "\n",
    "    # first_batch = next(iter(test_loader))\n",
    "    # with torch.inference_mode():\n",
    "    #     first_batch.to(device)\n",
    "    #     loaded_model_output = model_loaded(first_batch)\n",
    "\n",
    "    return model_loaded, optimizer, scheduler #, loaded_model_output\n",
    "\n",
    "basemodel_path = r\"/BASEMODEL_PATH.pth\"\n",
    "checkpoint_path = r\"/CHECKPOINT_PATH.pth\" \n",
    "model_loaded, optimizer, scheduler = load_checkpoint(basemodel_path, checkpoint_path, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(models, X_test, y_test):\n",
    "    f1_scores = {}\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        f1_scores[name] = f1\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(fpr, tpr, label=f'{name} ({roc_auc_score(y_test, y_pred_proba):.4f})')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(recall, precision, label=f'{name} ({average_precision_score(y_test, y_pred_proba):.4f})')\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    bars = plt.bar(f1_scores.keys(), f1_scores.values())\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.title('F1 scores - Baselines')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('AUROC - Baselines')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('AUPRC - Baselines')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_metrics(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(models, gt_model, test_loader, X_test, y_test, device='cuda'):\n",
    "    f1_scores = {}\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        f1_scores[name] = f1\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(fpr, tpr, label=f'{name} ({roc_auc_score(y_test, y_pred_proba):.4f})')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(recall, precision, label=f'{name} ({average_precision_score(y_test, y_pred_proba):.4f})')\n",
    "\n",
    "    test_acc, test_avg_loss, test_probs, test_preds, test_labels = test(test_loader, gt_model, device)\n",
    "    \n",
    "    f1_scores['GT'] = f1_gt\n",
    "    fpr_gt, tpr_gt, _ = roc_curve(test_labels, test_probs)\n",
    "    precision_gt, recall_gt, _ = precision_recall_curve(test_labels, test_probs)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(fpr_gt, tpr_gt, label=f'GT ({auc(fpr_gt, tpr_gt):.4f})', linestyle='--')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(recall_gt, precision_gt, label=f'GT ({auc(recall_gt, precision_gt):.4f})', linestyle='--')\n",
    "    \n",
    "    sorted_f1_scores = dict(sorted(f1_scores.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    bars = plt.bar(sorted_f1_scores.keys(), sorted_f1_scores.values())\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.title('F1 scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('AUROC')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('AUPRC')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(222)\n",
    "H = load_trans()\n",
    "train_loader, test_loader, val_loader = mask_and_batch_trans(H)\n",
    "plot_metrics(models, model_loaded, test_loader, X_test, y_test, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation-v1-VJG1ildO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
